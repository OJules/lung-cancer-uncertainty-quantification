# ğŸ¥ Uncertainty Quantification for Lung Cancer Prognosis

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Made with Jupyter](https://img.shields.io/badge/Made%20with-Jupyter-orange?logo=Jupyter)](https://jupyter.org/try)

**Master's Thesis Project**  
*University of NeuchÃ¢tel - Data Science & AI*  
**Author:** Jules Odje 
**Date:** November 2024

---

## ğŸ“– Table of Contents

- [Overview](#-overview)
- [Key Findings](#-key-findings)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [Results](#-results)
- [Methodology](#-methodology)
- [Citation](#-citation)
- [License](#-license)
- [Contact](#-contact)

---

## ğŸ¯ Overview

This project investigates **uncertainty quantification (UQ)** methods for machine learning models in medical prognosis, specifically focusing on lung cancer survival prediction. 

### Why This Matters

Machine learning models in healthcare often produce confident predictions without quantifying their uncertainty. This poses serious risks:
- **Overconfident predictions** can lead to incorrect treatment decisions
- **Lack of uncertainty awareness** prevents safe clinical deployment
- **No mechanism to flag ambiguous cases** for expert review

### Our Approach

We implement and compare three complementary uncertainty quantification methods:

1. **Conformal Prediction** - Provides prediction sets with mathematical coverage guarantees
2. **Bayesian Inference** - Quantifies uncertainty through probability distributions
3. **Model Calibration** - Ensures predicted probabilities reflect true likelihoods

---

## ğŸ”¥ Key Findings

### ğŸš¨ The Overconfidence Paradox

We discovered a **counterintuitive phenomenon**:

> Cases where BOTH UQ methods flagged as "LOW confidence" achieved **91.7% accuracy**  
> Cases where BOTH flagged as "HIGH confidence" achieved only **50% accuracy**

This reveals that:
- Models can be **dangerously overconfident** on difficult cases
- **Honest uncertainty** (P â‰ˆ 0.5) paradoxically indicates more reliable predictions
- Multiple UQ methods are **essential** to distinguish overconfidence from true uncertainty

### ğŸ“Š Performance Summary

| UQ Method | High Confidence Rate | Key Metric | Advantage |
|-----------|---------------------|------------|-----------|
| Conformal Prediction | 65.9% (29/44) | 68.2% coverage | Mathematical guarantees |
| Bayesian Inference | 25.0% (11/44) | Std = 0.261 | Full distributions |
| Calibration | Baseline best | Brier = 0.244 | Natural calibration |

### ğŸ¥ Clinical Impact

Based on our findings, we propose a **revised 3-tier clinical decision framework**:
```
âœ… TIER 1 (27%): Safe Automation - Both methods LOW confidence â†’ 92% accuracy
âš ï¸  TIER 2 (54%): Assisted Review - Methods disagree â†’ 50% accuracy  
ğŸš¨ TIER 3 (18%): Senior Escalation - Both methods HIGH confidence â†’ 50% accuracy
```

**Impact:** This inverted strategy achieves both higher safety (92% vs 50%) and greater efficiency (27% vs 18% automation).

---

## ğŸ“ Project Structure
```
lung-cancer-uncertainty-quantification/
â”‚
â”œâ”€â”€ README.md                          # ğŸ‘ˆ You are here
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ LICENSE                            # MIT License
â”‚
â”œâ”€â”€ data/                              # Dataset (not included for privacy)
â”‚   â”œâ”€â”€ README.md                      # Data description
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ notebooks/                         # ğŸ““ Jupyter/Colab Notebooks
â”‚   â”œâ”€â”€ 01_data_preprocessing.ipynb       # Data cleaning & feature engineering
â”‚   â”œâ”€â”€ 02_baseline_model.ipynb           # Random Forest baseline
â”‚   â”œâ”€â”€ 03_conformal_prediction.ipynb     # Conformal UQ method
â”‚   â”œâ”€â”€ 04_bayesian_inference.ipynb       # Bayesian UQ method
â”‚   â”œâ”€â”€ 05_calibration.ipynb              # Calibration analysis
â”‚   â””â”€â”€ 06_comparison.ipynb               # Comparative analysis
â”‚
â”œâ”€â”€ src/                               # ğŸ Python Source Code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py               # Data preprocessing functions
â”‚   â”œâ”€â”€ models.py                      # Model definitions
â”‚   â”œâ”€â”€ conformal.py                   # Conformal prediction implementation
â”‚   â”œâ”€â”€ bayesian.py                    # Bayesian inference implementation
â”‚   â””â”€â”€ calibration.py                 # Calibration methods
â”‚
â”œâ”€â”€ results/                           # ğŸ“Š Results & Outputs
â”‚   â”œâ”€â”€ README.md                      # â­ Detailed results documentation
â”‚   â”œâ”€â”€ figures/                       # All generated plots
â”‚   â”œâ”€â”€ tables/                        # CSV results tables
â”‚   â””â”€â”€ models/                        # Trained models (.pkl)
â”‚
â”œâ”€â”€ docs/                              # ğŸ“š Documentation
â”‚   â”œâ”€â”€ methodology.md                 # Detailed methodology
â”‚   â””â”€â”€ clinical_framework.md          # Clinical decision framework
â”‚
â””â”€â”€ tests/                             # âœ… Unit tests (optional)
    â””â”€â”€ test_conformal.py
```

---

## ğŸ”§ Installation

### Prerequisites

- Python 3.10 or higher
- pip package manager
- (Optional) Google Colab account for cloud execution

### Setup Instructions

1. **Clone the repository**
```bash
git clone https://github.com/OJules/lung-cancer-uncertainty-quantification.git
cd lung-cancer-uncertainty-quantification
```

2. **Create a virtual environment** (recommended)
```bash
# Using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Or using conda
conda create -n lung-cancer-uq python=3.10
conda activate lung-cancer-uq
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Verify installation**
```bash
python -c "import sklearn, numpy, pandas; print('âœ… Installation successful!')"
```

---

## ğŸš€ Usage

### Quick Start (Google Colab)

The easiest way to run the project is via Google Colab:

1. Open any notebook in `notebooks/`
2. Click "Open in Colab" badge
3. Run all cells

### Local Execution
```bash
# Navigate to notebooks directory
cd notebooks/

# Launch Jupyter
jupyter notebook

# Or use JupyterLab
jupyter lab
```

### Running the Full Pipeline

Execute notebooks in order:
```bash
01_data_preprocessing.ipynb      # ~2 minutes
02_baseline_model.ipynb          # ~3 minutes
03_conformal_prediction.ipynb    # ~2 minutes
04_bayesian_inference.ipynb      # ~2 minutes
05_calibration.ipynb             # ~1 minute
06_comparison.ipynb              # ~1 minute
```

**Total runtime:** ~15 minutes on Google Colab (free tier)

---

## ğŸ“Š Results

All results are documented in detail in [`results/README.md`](results/README.md).

### Quick Summary

**Baseline Model:**
- Accuracy: 61.4%
- AUC-ROC: 0.598
- Brier Score: 0.244

**Best UQ Configuration:**
- Conformal: 68.2% coverage (90% target)
- Bayesian: 25% high confidence (adaptive thresholds)
- Calibration: Already optimal (ECE = 0.042)

### Key Visualizations

<p align="center">
  <img src="results/figures/uq_methods_comparison.png" width="800" alt="UQ Methods Comparison">
  <br>
  <em>Figure 1: Comparison of three uncertainty quantification methods</em>
</p>

<p align="center">
  <img src="results/figures/uq_paradox_analysis.png" width="800" alt="Overconfidence Paradox">
  <br>
  <em>Figure 2: The overconfidence paradox - low confidence cases achieve higher accuracy</em>
</p>

For detailed results, see **[results/README.md](results/README.md)**.

---

## ğŸ”¬ Methodology

### Dataset

- **Source:** Lung Cancer Exploratory (LCE) dataset
- **Size:** 218 patients (174 training, 44 test)
- **Features:** 23 clinical and genomic features
- **Target:** 5-year survival (binary: long-term vs short-term)

### Models

**Baseline:** Random Forest Classifier
- n_estimators: 100
- max_depth: 5
- min_samples_split: 10

**Optimization:** RandomizedSearchCV with 3-fold cross-validation

### Uncertainty Quantification Methods

#### 1. Conformal Prediction
- **Framework:** Split conformal prediction
- **Significance levels:** Î± = 0.05, 0.10
- **Output:** Prediction sets {0}, {1}, or {0,1}

#### 2. Bayesian Inference
- **Approach:** Bootstrap aggregation via Random Forest trees
- **Output:** Probability distributions with mean, std, credible intervals
- **Thresholds:** Adaptive (percentile-based)

#### 3. Model Calibration
- **Methods:** Platt Scaling, Isotonic Regression
- **Metrics:** Brier Score, Expected Calibration Error (ECE)
- **Visualization:** Reliability diagrams

### Evaluation Metrics

- **Accuracy, AUC-ROC** (model performance)
- **Coverage, Set Size** (conformal prediction)
- **Std, Entropy, Credible Intervals** (Bayesian)
- **Brier Score, ECE** (calibration)

For complete methodology, see **[docs/methodology.md](docs/methodology.md)**.

---

## ğŸ“ Citation

If you use this work in your research, please cite:
```bibtex
@mastersthesis{kouakou2024uncertainty,
  title={Uncertainty Quantification for Machine Learning in Medical Prognosis: 
         A Case Study on Lung Cancer Survival Prediction},
  author={Odje, Jules },
  year={2024},
  school={University of NeuchÃ¢tel},
  type={Master's Thesis},
  address={NeuchÃ¢tel, Switzerland}
}
```

---

## ğŸ“š References

### Key Papers

1. **Conformal Prediction:**
   - Angelopoulos, A. N., & Bates, S. (2021). *A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification*. arXiv:2107.07511

2. **Calibration:**
   - Guo, C., et al. (2017). *On Calibration of Modern Neural Networks*. ICML 2017

3. **Bayesian Deep Learning:**
   - Kendall, A., & Gal, Y. (2017). *What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?* NIPS 2017

4. **Medical AI:**
   - Kompa, B., et al. (2021). *Second Opinion Needed: Communicating Uncertainty in Medical Machine Learning*. npj Digital Medicine

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to:

- ğŸ› Report bugs via [Issues](https://github.com/[username]/lung-cancer-uncertainty-quantification/issues)
- ğŸ’¡ Suggest improvements
- ğŸ”§ Submit pull requests

### Development Setup
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Check code style
flake8 src/
black src/
```

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

**Note:** The dataset is not included in this repository due to privacy considerations. Please refer to the original data source for access.

---

## ğŸ“§ Contact

**Jules Odje**

- ğŸ“ Master's Student in Data Science & AI
- ğŸ›ï¸ University of NeuchÃ¢tel, Switzerland
- ğŸ“§ Email: odjejulesgeraud@gmail.com
- ğŸ’¼ LinkedIn: [linkedin.com/in/jules-odje](https://linkedin.com/in/jules-odje)
- ğŸ™ GitHub: [@JulesOdje](https://github.com/OJules)

---

## ğŸ™ Acknowledgments

- **University of NeuchÃ¢tel** for academic support
- **cBioPortal** for providing the lung cancer dataset
- **Scikit-learn contributors** for excellent ML tools

---

## ğŸ“ˆ Project Status

- âœ… Data preprocessing completed
- âœ… Baseline model optimized
- âœ… Three UQ methods implemented
- âœ… Comparative analysis done
- âœ… Clinical framework proposed

**Last Updated:** November 2024

---

<p align="center">
  <strong>â­ If you find this project useful, please consider giving it a star! â­</strong>
</p>

<p align="center">
  Made with â¤ï¸ for safer medical AI
</p>
